tokenizer:
  alphabet: abcdefghijklmnopqrstuvwxyz0123456789-._
  vocab_size: 8000
  pad_token: '[PAD]'
  unk_token: '[UNK]'
  cls_token: '[CLS]'
  sep_token: '[SEP]'
  mask_token: '[MASK]'
  max_length: 256
  padding: true
  truncation: true
model:
  type: mbert
training:
  tokenizer:
    training_files:
    - C:\Users\tomic\ml\projects\dns_exfil_mbert\data\processed\train.txt
    - C:\Users\tomic\ml\projects\dns_exfil_mbert\data\processed\val.txt
    - C:\Users\tomic\ml\projects\dns_exfil_mbert\data\processed\test.txt
    save_dir: C:\Users\tomic\ml\projects\dns_exfil_mbert\artifacts\tokenizer\bpe\bpe8k
