tokenizer:
  alphabet: abcdefghijklmnopqrstuvwxyz0123456789-._
  vocab_size: 8192
  pad_token: '[PAD]'
  unk_token: '[UNK]'
  cls_token: '[CLS]'
  sep_token: '[SEP]'
  mask_token: '[MASK]'
  max_length: 256
  padding: true
  truncation: true
model:
  config_name: google-bert/bert-base-uncased
  tokenizer_name: BPETokenizer
training:
  tokenizer:
    training_files:
    - data/processed/train.txt
    - data/processed/val.txt
    - data/processed/test.txt
    save_dir: artifacts/tokenizer/bpe/bpe8k
  model:
    dropout: 0.1
data_collator:
  mlm_probability: 0.15
  data_collator:
    _target_: transformers.DataCollatorForLanguageModelling
    tokenizer: ${tokenizer}
train:
  output_dir: artifacts/bpe_mlm
  per_device_train_batch_size: 32
  gradient_accumulation_steps: 8
  learning_rate: 5.0e-05
  weight_decay: 0.01
  num_train_epochs: 3
  max_steps: 10000
  bf16: true
  logging_steps: 50
  save_steps: 1000
  save_total_limit: 3
  eval_strategy: steps
  eval_steps: 500
  label_smoothing_factor: 0.1
  mlm_probability: 0.15
  span_masking: false
  span_lambda: 3.0
  optimizer_type: adamw
  lr_scheduler_type: cosine
  warmup_ratio: 0.01
  gradient_checkpointing: true
  torch_compile: false
data:
  root: ${hydra:runtime.cwd}
  layout: processed
  block_size: 128
