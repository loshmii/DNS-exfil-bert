tokenizer:
  alphabet: abcdefghijklmnopqrstuvwxyz0123456789-._
  vocab_size: 8000
  pad_token: '[PAD]'
  unk_token: '[UNK]'
  cls_token: '[CLS]'
  sep_token: '[SEP]'
  mask_token: '[MASK]'
  max_length: 256
  padding: true
  truncation: true
model:
  type: mbert
training:
  tokenizer:
    training_files:
    - data/processed/train.txt
    - data/processed/val.txt
    - data/processed/test.txt
    save_dir: artifacts/tokenizer/bpe/bpe8k
  model:
    dropout: 0.1
data_collator:
  mlm_probability: 0.15
  data_collator:
    _target_: transformers.DataCollatorForLanguageModelling
    tokenizer: ${tokenizer}
