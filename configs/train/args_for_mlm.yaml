output_dir: experiments/toy_training
per_device_train_batch_size: 1
gradient_accumulation_steps: 1
learning_rate: 5e-5
weight_decay: 0.01
num_train_epochs: 3
max_steps: -1
bf16: true
logging_steps: 2
save_steps: 1000
save_total_limit: 3
eval_strategy: steps
eval_steps: 4
label_smoothing_factor: 0.1
report_to: none

mlm_probability: 0.15
span_masking: false
span_lambda: 3.0

optimizer_type: adafactor
lr_scheduler_type: cosine
warmup_ratio: 0.01

gradient_checkpointing: true
torch_compile: false